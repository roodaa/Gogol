"""
Point d'entr√©e principal pour Gogol

Ce script permet de lancer les diff√©rents composants du moteur de recherche:
- Crawler: T√©l√©charge des pages web
- Indexer: Construit l'index invers√© avec TF-IDF
- Search: Interface de recherche (√† venir)
"""

import argparse
import sys
from pathlib import Path

from src.crawler.crawler import Crawler
from src.indexer.indexer import Indexer


def crawl_command(args):
    """
    Lance le crawler avec les param√®tres sp√©cifi√©s.
    
    Args:
        args: Arguments de la ligne de commande
    """
    # Cr√©er le crawler avec max_pages personnalis√© si sp√©cifi√©
    if args.max_pages:
        crawler = Crawler(max_pages=args.max_pages)
    else:
        crawler = Crawler()  # Utilise la valeur par d√©faut de config.py
    
    # Lancer le crawling
    print(f"\nüï∑Ô∏è  Crawling de {args.url}")
    if args.max_pages:
        print(f"   Limite: {args.max_pages} pages")
    print()
    
    results = crawler.crawl(args.url)
    
    # Afficher le r√©sum√©
    print("\n" + "="*60)
    print("R√âSUM√â DU CRAWLING")
    print("="*60)
    print(f"Pages crawl√©es    : {results['pages_crawled']}")
    print(f"Pages sauvegard√©es: {results['pages_saved']}")
    print(f"Erreurs           : {results['errors']}")
    print("="*60)


def index_command(args):
    """
    Lance l'indexeur pour cr√©er l'index invers√©.
    
    Args:
        args: Arguments de la ligne de commande
    """
    print("\nüìö Indexation des documents")
    print()
    
    indexer = Indexer()
    
    # Forcer la reconstruction si demand√©
    force_rebuild = args.force if hasattr(args, 'force') else False
    
    stats = indexer.build_index(force_rebuild=force_rebuild)
    
    print("\n‚úì Indexation termin√©e!")


def search_command(args):
    """
    Lance une recherche (√† impl√©menter).
    
    Args:
        args: Arguments de la ligne de commande
    """
    from src.search_engine.query_processor import QueryProcessor
    from src.search_engine.ranker import Ranker
    
    print(f"\nüîç Recherche: '{args.query}'")
    print()
    
    # Initialiser le moteur
    processor = QueryProcessor()
    ranker = Ranker()
    
    # Traiter la requ√™te
    terms = processor.process(args.query)
    print(f"Termes normalis√©s: {terms}")
    
    if not terms:
        print("‚ö†Ô∏è  Aucun terme valide dans la requ√™te")
        return
    
    # Rechercher
    results = ranker.rank(terms, top_k=args.top if hasattr(args, 'top') else 10)
    
    # Afficher les r√©sultats
    if results:
        print(f"\nR√©sultats ({len(results)} trouv√©s):\n")
        for i, (doc_id, title, url, score) in enumerate(results, 1):
            print(f"{i}. [{score:.4f}] {title}")
            print(f"   {url}\n")
    else:
        print("‚ùå Aucun r√©sultat trouv√©")


def main():
    """
    Point d'entr√©e principal avec parsing des arguments.
    """
    parser = argparse.ArgumentParser(
        description='Gogol - Moteur de recherche √©ducatif',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Crawler 50 pages depuis Wikipedia
  python main.py crawl --url https://fr.wikipedia.org/wiki/Python --max-pages 50
  
  # Crawler avec la limite par d√©faut (100 pages)
  python main.py crawl --url https://fr.wikipedia.org/wiki/France
  
  # Indexer les documents crawl√©s
  python main.py index
  
  # Forcer la reconstruction de l'index
  python main.py index --force
  
  # Rechercher
  python main.py search "Python programming"
  
  # Rechercher avec plus de r√©sultats
  python main.py search "machine learning" --top 20
        """
    )
    
    # Sous-commandes
    subparsers = parser.add_subparsers(dest='command', help='Commande √† ex√©cuter')
    
    # ========================================================================
    # Commande: crawl
    # ========================================================================
    crawl_parser = subparsers.add_parser(
        'crawl',
        help='Crawler des pages web'
    )
    crawl_parser.add_argument(
        '--url',
        type=str,
        required=True,
        help="URL de d√©part pour le crawling"
    )
    crawl_parser.add_argument(
        '--max-pages',
        type=int,
        default=None,
        help="Nombre maximum de pages √† crawler (d√©faut: 100)"
    )
    
    # ========================================================================
    # Commande: index
    # ========================================================================
    index_parser = subparsers.add_parser(
        'index',
        help='Indexer les documents crawl√©s'
    )
    index_parser.add_argument(
        '--force',
        action='store_true',
        help="Forcer la reconstruction de l'index (supprime l'ancien)"
    )
    
    # ========================================================================
    # Commande: search
    # ========================================================================
    search_parser = subparsers.add_parser(
        'search',
        help='Effectuer une recherche'
    )
    search_parser.add_argument(
        'query',
        type=str,
        help="Requ√™te de recherche"
    )
    search_parser.add_argument(
        '--top',
        type=int,
        default=10,
        help="Nombre de r√©sultats √† afficher (d√©faut: 10)"
    )
    
    # Parser les arguments
    args = parser.parse_args()
    
    # Ex√©cuter la commande appropri√©e
    if args.command == 'crawl':
        crawl_command(args)
    elif args.command == 'index':
        index_command(args)
    elif args.command == 'search':
        search_command(args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()

# Gogol - Moteur de Recherche

Un moteur de recherche dÃ©veloppÃ© de maniÃ¨re progressive, inspirÃ© de Google.

## Description

Gogol est un projet Ã©ducatif visant Ã  comprendre et implÃ©menter les concepts fondamentaux d'un moteur de recherche :
- Crawling de pages web
- Indexation de contenu
- Algorithmes de recherche et ranking
- Interface utilisateur web

## Structure du Projet

```
gogol/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler/          # Module de crawling web
â”‚   â”œâ”€â”€ indexer/          # Indexation et stockage des documents
â”‚   â”œâ”€â”€ search_engine/    # Logique de recherche et ranking
â”‚   â””â”€â”€ web_interface/    # API et interface web
â”œâ”€â”€ tests/                # Tests unitaires et d'intÃ©gration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # DonnÃ©es brutes crawlÃ©es
â”‚   â””â”€â”€ indexed/         # Index de recherche
â”œâ”€â”€ logs/                # Fichiers de logs
â””â”€â”€ requirements.txt     # DÃ©pendances Python
```

## Roadmap

### Version 1.0 - Moteur de Recherche Basique
- [x] Crawler simple pour fichiers HTML (101 pages Wikipedia FR crawlÃ©es)
- [x] Indexation par mots-clÃ©s (inverted index avec TF-IDF)
- [ ] Recherche basique par correspondance de termes
- [ ] Interface web minimaliste

### Version 2.0 - AmÃ©liorations
- [ ] Crawling de pages web rÃ©elles
- [ ] Ranking TF-IDF
- [ ] Support de recherche multi-mots
- [ ] AmÃ©lioration de l'interface

### Version 3.0 - FonctionnalitÃ©s AvancÃ©es
- [ ] ImplÃ©mentation de PageRank
- [ ] Recherche full-text avec Elasticsearch
- [ ] Crawling distribuÃ©
- [ ] Suggestions de recherche

## Installation

1. Cloner le repository
```bash
git clone <url>
cd gogol
```

2. CrÃ©er un environnement virtuel
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

## Utilisation

### 1. Crawling de pages web

Crawler des pages web Ã  partir d'une URL de dÃ©part :

```bash
python main.py crawl --url https://fr.wikipedia.org/wiki/France
```

Le crawler :
- TÃ©lÃ©charge les pages HTML
- Extrait le contenu textuel et les liens
- Sauvegarde les donnÃ©es en JSON dans `data/raw/`
- Respecte un dÃ©lai entre les requÃªtes (politesse)

**DonnÃ©es actuelles** : 101 pages Wikipedia FR (~66 MB)

### 2. Indexation des documents

CrÃ©er l'index inversÃ© Ã  partir des documents crawlÃ©s :

```bash
# Indexer tous les documents
python main.py index

# Reconstruire l'index complet
python main.py index --force

# Afficher les statistiques de l'index
python main.py index --stats
```

**RÃ©sultats de l'indexation** :
- ğŸ“„ **101 documents** indexÃ©s
- ğŸ”¤ **6,368 termes uniques** (vocabulaire franÃ§ais)
- ğŸ“Š **426,168 postings** (entrÃ©es dans l'index inversÃ©)
- ğŸ’¾ **48.12 MB** de base de donnÃ©es SQLite
- âš¡ Temps d'indexation : ~8 minutes

#### Comment fonctionne l'indexeur ?

L'indexeur transforme les documents bruts en un index inversÃ© recherchable :

**1. Traitement du texte franÃ§ais**
- Tokenisation avec NLTK (`word_tokenize`)
- Normalisation : lowercase, suppression ponctuation
- Filtrage par longueur (3-50 caractÃ¨res)
- Suppression de 157 stop words franÃ§ais ("le", "la", "de", etc.)
- Stemming avec FrenchStemmer ("chÃ¢teaux" â†’ "chÃ¢teau")

**2. Construction de l'index inversÃ©**

L'index inversÃ© permet de rechercher rapidement : "Quels documents contiennent ce terme ?"

Structure en 4 tables SQLite :

```
Documents (101 entrÃ©es)
â”œâ”€â”€ url, title, doc_hash
â”œâ”€â”€ text_length, term_count
â””â”€â”€ indexed_at

Terms (6,368 entrÃ©es)
â”œâ”€â”€ term (mot normalisÃ©)
â”œâ”€â”€ document_frequency (nombre de docs contenant ce terme)
â””â”€â”€ total_occurrences

Postings (426,168 entrÃ©es) - Index inversÃ©
â”œâ”€â”€ term_id â†’ doc_id (relation)
â”œâ”€â”€ term_frequency (frÃ©quence dans le document)
â”œâ”€â”€ tf_idf_score (score de pertinence prÃ©-calculÃ©)
â””â”€â”€ positions (JSON des positions pour phrase search)

IndexMetadata
â””â”€â”€ Statistiques globales (total_docs, etc.)
```

**3. Scoring TF-IDF**

Chaque terme reÃ§oit un score de pertinence par document :

```
TF (Term Frequency) = occurrences du terme / total de termes dans le doc
IDF (Inverse Document Frequency) = log(total docs / docs contenant le terme)
TF-IDF = TF Ã— IDF
```

Les scores TF-IDF sont **prÃ©-calculÃ©s** et stockÃ©s dans la base pour des recherches ultra-rapides.

**Exemple de traitement** :

```
Texte original : "Les chÃ¢teaux de la Loire sont magnifiques"

â†“ Tokenisation
["Les", "chÃ¢teaux", "de", "la", "Loire", "sont", "magnifiques"]

â†“ Normalisation + Stop words
["chÃ¢teaux", "Loire", "magnifiques"]

â†“ Stemming
["chÃ¢teau", "loir", "magnifiq"]

â†“ Stockage dans l'index inversÃ©
"chÃ¢teau" â†’ Document #42 (TF=0.05, IDF=2.3, TF-IDF=0.115)
"loir" â†’ Document #42 (TF=0.02, IDF=4.1, TF-IDF=0.082)
"magnifiq" â†’ Document #42 (TF=0.01, IDF=1.8, TF-IDF=0.018)
```

### 3. Recherche (Ã  venir)

```bash
python main.py search --query "chÃ¢teaux de la Loire"
```

### 4. Interface web (Ã  venir)

```bash
python main.py web
```

DÃ©marrera l'interface web sur http://127.0.0.1:8000

## Technologies

- **Python 3.14** - Langage principal
- **FastAPI** - Framework web (Ã  venir)
- **BeautifulSoup** - Parsing HTML pour le crawler
- **SQLAlchemy** - ORM pour la base de donnÃ©es SQLite
- **NLTK** - Traitement du langage naturel franÃ§ais
  - FrenchStemmer (Snowball)
  - Stop words franÃ§ais (157 mots)
  - Tokenisation `word_tokenize`
- **SQLite** - Base de donnÃ©es de l'index inversÃ©

## Architecture de l'Indexeur

```
Fichiers JSON (data/raw/)
         â†“
    [Indexer]
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Chargement JSON    â”‚
â”‚  2. Tokenisation NLTK  â”‚
â”‚  3. Normalisation      â”‚
â”‚     - Lowercase        â”‚
â”‚     - Stop words       â”‚
â”‚     - Stemming         â”‚
â”‚  4. Calcul statistiquesâ”‚
â”‚  5. Stockage SQLite    â”‚
â”‚  6. Calcul TF-IDF      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
Base de donnÃ©es SQLite
(gogol_index.db)
         â†“
PrÃªt pour recherche!
```

### Fichiers du module indexer

- **`src/indexer/models.py`** - ModÃ¨les SQLAlchemy (Document, Term, Posting, IndexMetadata)
- **`src/indexer/indexer.py`** - Classe Indexer avec la logique de traitement
- **`docs/database_schema.puml`** - Diagramme PlantUML de la base de donnÃ©es

### Exemples de requÃªtes sur l'index

Vous pouvez interroger directement la base de donnÃ©es SQLite pour explorer l'index :

```bash
# Se connecter Ã  la base de donnÃ©es
sqlite3 data/indexed/gogol_index.db
```

**Exemples de requÃªtes SQL** :

```sql
-- Termes les plus frÃ©quents
SELECT term, total_occurrences, document_frequency
FROM terms
ORDER BY total_occurrences DESC
LIMIT 10;

-- Documents indexÃ©s
SELECT title, url, term_count, text_length
FROM documents
ORDER BY term_count DESC
LIMIT 5;

-- Rechercher un terme spÃ©cifique
SELECT d.title, d.url, p.tf_idf_score
FROM postings p
JOIN terms t ON p.term_id = t.id
JOIN documents d ON p.doc_id = d.id
WHERE t.term = 'chÃ¢teau'
ORDER BY p.tf_idf_score DESC
LIMIT 10;

-- Statistiques globales
SELECT
  (SELECT COUNT(*) FROM documents) as total_docs,
  (SELECT COUNT(*) FROM terms) as total_terms,
  (SELECT COUNT(*) FROM postings) as total_postings;
```

## Auteur

Romaric Dacosse - Ã‰tudiant IngÃ©nieur UTC (GI04)
- SpÃ©cialisation : IA, Analyse Data, Machine Learning

## Licence

Projet Ã©ducatif - UTC 2024-2025
